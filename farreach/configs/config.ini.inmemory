[global]
;workload_name=workloada
workload_name=synthetic
; workload_mode: 0 - static mode; 1 - dynamic mode
workload_mode=1
; 6*10 seconds
dynamic_periodnum=3
dynamic_periodinterval=10
; dynamicrules/synthetic-hotin.out{1, 2, ..., period_num}
dynamic_ruleprefix=hotin
; max # of bytes in each value
max_vallen=128
; data size of each batch when reading files to load workload
load_batch_size=10000
; # of physical clients (update MAT size correspondingly)
client_physical_num=2
; # of physical servers (update MAT size correspondingly)
;4
server_physical_num=2
; total # of client threads
client_total_logical_num=256
; total # of server threads (all server logical idxes must < server_total_logical_num)
; change MAX_SERVER_NUM in tofino/netbufferv4.p4 if necessary
;128
server_total_logical_num=2
; only used by remote_client for server rotation under static workload
server_total_logical_num_for_rotation=128
; bottleneck server logical idx (bottleneck partition) for server rotation (change test_server_rotation.sh accordingly)
bottleneck_serveridx_for_rotation=118

[client]
; a single port (the first physical client uses the port the receive rotation data from other physical clients)
client_rotationdataserver_port=1025
; [client_sendpktserver_port_start, client_sendpktserver_port_start + client_physical_num - 1]
; NOTE: launch other physical clients first; after they are waiting for can_sendpkt, launch the first physical client (client 0) to tell them that they can send pkt at the same time
client_sendpktserver_port_start=1026
; [client_rulemapserver_port_start, client_rulemapserver_port_start + client_physical_num - 1]
; non-first physical clients wait for commands from the first physical client to switch dynamic rulemap to simulate key popularity change
client_rulemapserver_port_start=1036
; [client_worker_port_start, client_worker_port_start + local client logical num - 1]
client_worker_port_start=8888

[client0]
; client thread num for transaction phase in remote client
client_logical_num=128
; dl11 -> bf1 (dpdk interface)
client_ip=10.0.1.1
client_mac=TODO
client_fpport=1/0
client_pipeidx=1
client_ip_for_client0=192.168.0.110

[client1]
; client thread num for transaction phase in remote client
client_logical_num=128
; dl15 -> bf1 (dpdk interface)
client_ip=10.0.1.2
client_mac=TODO
client_fpport=3/0
client_pipeidx=1
client_ip_for_client0=192.168.0.120

; common configuration of all physical servers
[server]
; # of loading threads for per-server db (not used in ycsb benchmark)
server_load_factor=5
; [server_worker_port_start, server_worker_port_start + local server logical num - 1]
; reserve 1152 (0x480) ~ 1279 (0x4FF), which & 0xF80 = 0x480, for logical server simulation per physical server
; NOTE: update p4src/parser accordingly
server_worker_port_start=1152
; [server_evictserver_port_start, server_evictserver_port_start + max global server logical idx]
server_evictserver_port_start=2222
; [server_snapshotserver_port_start, server_snapshotserver_port_start + max global server logical idx]
server_snapshotserver_port_start=3333
; [server_snapshotdataserver_port_start, server_snapshotdataserver_port_start + max global server logical idx]
server_snapshotdataserver_port_start=4444
; a single port
transaction_loadfinishserver_port=8000

[server0]
; # of cores used by server.workers, which must <= server_total_corenum but >= # of server_logical_idxes
; NOTE: if # of server_logical_idxes > server_worker_corenum, we report a warning and bind extra server threads to the limited server_cores circularly
server_worker_corenum=32
server_total_corenum=40
; all global server logical idxes must < max_server_thread_num
;0:1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31
server_logical_idxes=0
; dl13 -> bf1 (dpdk interface)
server_ip=10.0.1.3
server_mac=TODO
server_fpport=5/0
server_pipeidx=1
; NOTE: not use the NIC corresponding to dpdk interface of server
server_ip_for_controller=192.168.0.130

[server1]
; # of cores used by server.workers, which must <= server_total_corenum but >= # of server_logical_idxes
; NOTE: if # of server_logical_idxes > server_worker_corenum, we report a warning and bind extra server threads to the limited server_cores circularly
server_worker_corenum=32
server_total_corenum=40
; all global server logical idxes must < max_server_thread_num
;32:33:34:35:36:37:38:39:40:41:42:43:44:45:46:47:48:49:50:51:52:53:54:55:56:57:58:59:60:61:62:63
server_logical_idxes=1
; dl16 -> bf1 (dpdk interface)
server_ip=10.0.1.4
server_mac=TODO
server_fpport=2/0
server_pipeidx=1
; NOTE: not use the NIC corresponding to dpdk interface of server
server_ip_for_controller=192.168.0.140

[server2]
; # of cores used by server.workers, which must <= server_total_corenum but >= # of server_logical_idxes
; NOTE: if # of server_logical_idxes > server_worker_corenum, we report a warning and bind extra server threads to the limited server_cores circularly
server_worker_corenum=32
server_total_corenum=40
; all global server logical idxes must < max_server_thread_num
server_logical_idxes=64:65:66:67:68:69:70:71:72:73:74:75:76:77:78:79:80:81:82:83:84:85:86:87:88:89:90:91:92:93:94:95
; dl16 -> bf1 (dpdk interface)
server_ip=10.0.1.5
server_mac=TODO
server_fpport=4/0
server_pipeidx=1
; NOTE: not use the NIC corresponding to dpdk interface of server
server_ip_for_controller=192.168.0.150

[server3]
; # of cores used by server.workers, which must <= server_total_corenum but >= # of server_logical_idxes
; NOTE: if # of server_logical_idxes > server_worker_corenum, we report a warning and bind extra server threads to the limited server_cores circularly
server_worker_corenum=32
server_total_corenum=40
; all global server logical idxes must < max_server_thread_num
server_logical_idxes=96:97:98:99:100:101:102:103:104:105:106:107:108:109:110:111:112:113:114:115:116:117:118:119:120:121:122:123:124:125:126:127
; dl16 -> bf1 (dpdk interface)
server_ip=10.0.1.6
server_mac=TODO
server_fpport=6/0
server_pipeidx=1
; NOTE: not use the NIC corresponding to dpdk interface of server
server_ip_for_controller=192.168.0.160

[controller]
controller_ip_for_server=192.168.0.130
controller_ip_for_switchos=192.168.0.130
; [controller_popserver_port_start, controller_popserver_port_start + max serveridx]
controller_popserver_port_start=5555
; a single port
controller_evictserver_port=5002
; unit: ms
controller_snapshot_period=10000
; a single port
controller_warmupfinishserver_port=5010

[switch]
; hash partition range / virtual partition num
; change PARTITION_COUNT in tofino/netbufferv4.p4 accordingly (only for hash partition)
switch_partition_count=32768
; change KV_BUCKET_COUNT in tofino/netbufferv4.p4 accordingly (one stage can hold at most 32700 instead of 32768)
;switch_kv_bucket_num=32700
switch_kv_bucket_num=10000
; max # of pipelines in switch
switch_pipeline_num=2
; max # of bytes supported in switch (8B aligned)
switch_max_vallen=128
switchos_ip=192.168.0.244
switchos_sample_cnt=10
; a single port
switchos_popserver_port=5003
; a single port
switchos_snapshotserver_port=5004
; a single port
switchos_specialcaseserver_port=5005
; a single port
switchos_ptf_popserver_port=5006
; a single port
switchos_ptf_snapshotserver_port=5007
; 2/0 of pipeline 1 connects with 12/0 of pipeline 0 which simulates cross-ingress-pipeline switching by hardware link
switch_recirport_pipeline1to0=NONE
switch_recirport_pipeline0to1=NONE

; NOTE: we place reflector at the first physical server
[reflector]
reflector_ip_for_switchos=192.168.0.130
; a single port (NOTE: udpate p4src/parser.p4 accordingly)
reflector_dp2cpserver_port=5008
; a single port (NOTE: udpate p4src/parser.p4 accordingly)
reflector_cp2dpserver_port=5009
